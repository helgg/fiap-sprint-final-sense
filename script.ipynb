{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f44a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEOFError: marshal data too short. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import locale\n",
    "import os\n",
    "locale.setlocale(locale.LC_ALL,'pt_BR.UTF-8')\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.cluster import KMeans\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f_oneway\n",
    "import gc\n",
    "gc.collect()\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f9f46",
   "metadata": {},
   "source": [
    "## Início do modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa090c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEOFError: marshal data too short. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) Diretórios básica\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(r\"Data\\parquet\") \n",
    "OUT_DIR  = Path(r\"Output\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Lista de arquivos\n",
    "arquivos = {\n",
    "    # centrais\n",
    "    \"dados_clientes\": DATA_DIR / \"dados_clientes.parquet\",\n",
    "    \"clientes_desde\": DATA_DIR / \"clientes_desde.parquet\",\n",
    "    \"contratacoes_12m\": DATA_DIR / \"contratacoes_ultimos_12_meses.parquet\",\n",
    "    \"historico\": DATA_DIR / \"historico.parquet\",\n",
    "    \"mrr\": DATA_DIR / \"mrr.parquet\",\n",
    "    \"tickets\": DATA_DIR / \"tickets.parquet\",\n",
    "    # NPS\n",
    "    \"nps_relacional\": DATA_DIR / \"nps_relacional.parquet\",\n",
    "    \"nps_tx_aquisicao\": DATA_DIR / \"nps_transacional_aquisicao.parquet\",\n",
    "    \"nps_tx_implantacao\": DATA_DIR / \"nps_transacional_implantacao.parquet\",\n",
    "    \"nps_tx_onboarding\": DATA_DIR / \"nps_transacional_onboarding.parquet\",\n",
    "    \"nps_tx_produto\": DATA_DIR / \"nps_transacional_produto.parquet\",\n",
    "    \"nps_tx_suporte\": DATA_DIR / \"nps_transacional_suporte.parquet\",\n",
    "    # telemetria\n",
    "    **{f\"telemetria_{i}\": DATA_DIR / f\"telemetria_{i}.parquet\" for i in range(1, 12)}\n",
    "}\n",
    "\n",
    "def ler_parquet(p: Path):\n",
    "    try:\n",
    "        return pd.read_parquet(p)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalizar_colunas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\").lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2) Leitura e união por família de dataframe\n",
    "# -------------------------------------------\n",
    "tabelas = {}\n",
    "for nome, caminho in arquivos.items():\n",
    "    df = ler_parquet(caminho)\n",
    "    if df is not None and len(df.columns) > 0:\n",
    "        tabelas[nome] = normalizar_colunas(df)\n",
    "\n",
    "# Une telemetria\n",
    "telemetria = pd.concat([df for k, df in tabelas.items() if k.startswith(\"telemetria_\")], ignore_index=True) \\\n",
    "             if any(k.startswith(\"telemetria_\") for k in tabelas) else None\n",
    "\n",
    "# Une NPS transacional\n",
    "nps_trans = pd.concat([tabelas[k] for k in tabelas if k.startswith(\"nps_tx_\")], ignore_index=True) \\\n",
    "           if any(k.startswith(\"nps_tx_\") for k in tabelas) else None\n",
    "\n",
    "# NPS relacional\n",
    "nps_rel = tabelas.get(\"nps_relacional\")\n",
    "\n",
    "# Outras bases\n",
    "dados_clientes   = tabelas.get(\"dados_clientes\")\n",
    "clientes_desde   = tabelas.get(\"clientes_desde\")\n",
    "contratacoes_12m = tabelas.get(\"contratacoes_12m\")\n",
    "historico        = tabelas.get(\"historico\")\n",
    "mrr              = tabelas.get(\"mrr\")\n",
    "tickets          = tabelas.get(\"tickets\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Achando colunas-chave (simples)\n",
    "# -----------------------------\n",
    "# Tentamos nomes comuns para CID, DATA, VALOR — se não achar, usamos casamento simples de nome de colunas\n",
    "def coluna_cliente(df):\n",
    "    if df is None: return None\n",
    "    for c in [\"cod_cliente\", \"cd_cliente\", \"metadata_codcliente\", \"cid\", \"cliente_id\", \"id_cliente\"]:\n",
    "        if c in df.columns: return c\n",
    "\n",
    "    for c in df.columns:\n",
    "        if \"cliente\" in c or c.endswith(\"cid\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coluna_data(df):\n",
    "    if df is None: return None\n",
    "    for c in [\"purchase_date\",\"respondedat\",\"responded_at\",\"data\",\"dt_evento\",\"created_at\",\"dt\",\"mes\",\"competencia\"]:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c or \"data\" in c or c.startswith(\"dt\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coluna_valor(df):\n",
    "    if df is None: return None\n",
    "    for c in [\"net_amount\",\"valor\",\"vlr\",\"mrr\",\"valor_total\",\"amount\",\"gross_amount\"]:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if (\"valor\" in c) or (\"amount\" in c) or (\"mrr\" in c):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Monta EVENTS (para recency/frequency)\n",
    "# -----------------------------\n",
    "def montar_events(lista_df):\n",
    "    frames = []\n",
    "    for nome, df in lista_df:\n",
    "        if df is None: \n",
    "            continue\n",
    "        cid = coluna_cliente(df)\n",
    "        dt  = coluna_data(df)\n",
    "        if cid and dt and cid in df.columns and dt in df.columns:\n",
    "            tmp = df[[cid, dt]].copy()\n",
    "            tmp.columns = [\"cid\", \"event_dt\"]\n",
    "            tmp[\"cid\"] = tmp[\"cid\"].astype(str)\n",
    "            tmp[\"event_dt\"] = pd.to_datetime(tmp[\"event_dt\"], errors=\"coerce\")\n",
    "            tmp = tmp.dropna(subset=[\"cid\", \"event_dt\"])\n",
    "            frames.append(tmp)\n",
    "    if frames:\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"cid\", \"event_dt\"])\n",
    "\n",
    "events = montar_events([\n",
    "    (\"nps_rel\", nps_rel),\n",
    "    (\"nps_trans\", nps_trans),\n",
    "    (\"historico\", historico),\n",
    "    (\"mrr\", mrr),\n",
    "    (\"telemetria\", telemetria),\n",
    "    (\"tickets\", tickets)\n",
    "])\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Monta MONETARY (somas por cliente)\n",
    "# -----------------------------\n",
    "def montar_monetary(lista_df):\n",
    "    frames = []\n",
    "    for nome, df in lista_df:\n",
    "        if df is None:\n",
    "            continue\n",
    "        cid = coluna_cliente(df)\n",
    "        val = coluna_valor(df)\n",
    "        if cid and val and cid in df.columns and val in df.columns:\n",
    "            tmp = df[[cid, val]].copy()\n",
    "            tmp.columns = [\"cid\", \"amount\"]\n",
    "            tmp[\"cid\"]   = tmp[\"cid\"].astype(str)\n",
    "            tmp[\"amount\"] = pd.to_numeric(tmp[\"amount\"], errors=\"coerce\")\n",
    "            frames.append(tmp)\n",
    "    if frames:\n",
    "        out = pd.concat(frames, ignore_index=True)\n",
    "        out[\"amount\"] = out[\"amount\"].fillna(0.0)\n",
    "        return out\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"cid\", \"amount\"])\n",
    "\n",
    "monetary = montar_monetary([\n",
    "    (\"mrr\", mrr),\n",
    "    (\"contratacoes_12m\", contratacoes_12m),\n",
    "    (\"historico\", historico)\n",
    "])\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Calcula RFM (Recency, Frequency, Monetary):\n",
    "# -----------------------------\n",
    "# Identifica comportamento, último evento, frequência e valores\n",
    "if len(events) > 0:\n",
    "    ref_date = events[\"event_dt\"].max() + pd.Timedelta(days=1)\n",
    "    last_evt = events.groupby(\"cid\", as_index=False)[\"event_dt\"].max().rename(columns={\"event_dt\":\"last_event\"})\n",
    "    freq     = events.groupby(\"cid\", as_index=False).size().rename(columns={\"size\":\"frequency\"})\n",
    "    rfm = last_evt.merge(freq, on=\"cid\", how=\"outer\")\n",
    "else:\n",
    "    ref_date = pd.Timestamp.today()\n",
    "    rfm = pd.DataFrame(columns=[\"cid\",\"last_event\",\"frequency\"])\n",
    "\n",
    "if len(monetary) > 0:\n",
    "    money = monetary.groupby(\"cid\", as_index=False)[\"amount\"].sum().rename(columns={\"amount\":\"monetary\"})\n",
    "    rfm = rfm.merge(money, on=\"cid\", how=\"left\")\n",
    "else:\n",
    "    rfm[\"monetary\"] = 0.0\n",
    "\n",
    "rfm[\"last_event\"] = pd.to_datetime(rfm.get(\"last_event\"), errors=\"coerce\")\n",
    "rfm[\"recency\"]    = (ref_date - rfm[\"last_event\"]).dt.days\n",
    "rfm[\"frequency\"]  = pd.to_numeric(rfm.get(\"frequency\"), errors=\"coerce\").fillna(0).astype(int)\n",
    "rfm[\"monetary\"]   = pd.to_numeric(rfm.get(\"monetary\"), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "\n",
    "rfm.to_csv(OUT_DIR / \"rfm_raw.csv\", index=False)\n",
    "\n",
    "rfm_model = rfm.dropna(subset=[\"recency\"]).copy()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) KMeans (k=3) – simples\n",
    "# -----------------------------\n",
    "if len(rfm_model) >= 3:\n",
    "    # log1p para suavizar, alterado pelo Helder\n",
    "    X = np.log1p(rfm_model[[\"recency\",\"frequency\",\"monetary\"]].values)   \n",
    "    X = StandardScaler().fit_transform(X)                               \n",
    "    km = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X)\n",
    "    rfm_model[\"cluster\"] = km.labels_\n",
    "\n",
    "\n",
    "    profiles = (rfm_model.groupby(\"cluster\")\n",
    "                .agg(recency_mean=(\"recency\",\"mean\"),\n",
    "                     frequency_mean=(\"frequency\",\"mean\"),\n",
    "                     monetary_mean=(\"monetary\",\"mean\"),\n",
    "                     customers=(\"cid\",\"nunique\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"recency_mean\",\"frequency_mean\",\"monetary_mean\"],\n",
    "                             ascending=[True, False, False]))\n",
    "\n",
    "    # rótulos mais amigáveis (colunas semelhantes feitas pela Liora)\n",
    "    ordem = profiles[\"cluster\"].tolist()\n",
    "    nomes = [\"Best\",\"Loyal\",\"At Risk\"]  # se k=3\n",
    "    mapa = {cl: nome for cl, nome in zip(ordem, nomes)}\n",
    "    rfm_model[\"segment\"] = rfm_model[\"cluster\"].map(mapa)\n",
    "else:\n",
    "    profiles = pd.DataFrame(columns=[\"cluster\",\"recency_mean\",\"frequency_mean\",\"monetary_mean\",\"customers\"])\n",
    "    rfm_model[\"cluster\"] = np.nan\n",
    "    rfm_model[\"segment\"] = np.nan\n",
    "\n",
    "rfm_model.to_csv(OUT_DIR / \"rfm_with_clusters.csv\", index=False)\n",
    "profiles.to_csv(OUT_DIR / \"cluster_profiles.csv\", index=False)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# --------------------------------------\n",
    "# 8) Tendência mensal de eventos + flag de alerta\n",
    "# --------------------------------------\n",
    "\n",
    "if len(events) > 0:\n",
    "    events = events.copy()\n",
    "    events['event_dt'] = pd.to_datetime(events['event_dt'], errors='coerce')\n",
    "    events = events.dropna(subset=['event_dt'])\n",
    "\n",
    "\n",
    "    # mask = (events['event_dt'] >= '2024-01-01') & (events['event_dt'] < '2025-01-01') ## não esta muito bom isso ainda\n",
    "    # events = events.loc[mask]\n",
    "\n",
    "    # Agrega por mês-calendário (MS = Month Start) e normaliza para \"1º dia do mês\"\n",
    "    monthly = (\n",
    "        events.set_index('event_dt')\n",
    "              .groupby(pd.Grouper(freq='MS'))\n",
    "              .agg(events=('cid', 'count'))\n",
    "              .reset_index()\n",
    "              .rename(columns={'event_dt': 'month'})\n",
    "    )\n",
    "    monthly['month'] = pd.to_datetime(monthly['month'], errors='coerce')\n",
    "    monthly = monthly.dropna(subset=['month']).sort_values('month')\n",
    "    monthly['month'] = monthly['month'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "    # janela 6 meses\n",
    "    monthly['ma'] = monthly['events'].rolling(6, min_periods=3).mean()\n",
    "    monthly['sd'] = monthly['events'].rolling(6, min_periods=3).std()\n",
    "    monthly['lo_band'] = monthly['ma'] - 2 * monthly['sd']\n",
    "    monthly['anomaly_low'] = monthly['events'] < monthly['lo_band']\n",
    "\n",
    "else:\n",
    "    monthly = pd.DataFrame(columns=['month', 'events', 'ma', 'sd', 'lo_band', 'anomaly_low'])\n",
    "\n",
    "monthly.to_csv(OUT_DIR / 'monthly_events_trend.csv', index=False)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -----------\n",
    "# 9) Gráficos \n",
    "# -----------\n",
    "# 9.1 média monetária por cluster\n",
    "if len(profiles) > 0:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(profiles[\"cluster\"].astype(str), profiles[\"monetary_mean\"])\n",
    "    plt.title(\"Cluster monetary mean\")\n",
    "    plt.xlabel(\"Cluster\"); plt.ylabel(\"Monetary (mean)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"chart_cluster_monetary_mean.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 9.2 dispersão Frequency x Monetary\n",
    "if \"cluster\" in rfm_model.columns and rfm_model[\"cluster\"].notna().any():\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(np.log1p(rfm_model[\"frequency\"]), np.log1p(rfm_model[\"monetary\"]))\n",
    "    plt.title(\"Log(Frequency) vs Log(Monetary)\")\n",
    "    plt.xlabel(\"log(Frequency)\")\n",
    "    plt.ylabel(\"log(Monetary)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"chart_scatter_freq_monetary.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 9.3 tendência mensal de eventos\n",
    "if len(monthly) > 0:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(monthly[\"event_dt\"], monthly[\"events\"])\n",
    "    plt.title(\"Monthly Events Trend\")\n",
    "    plt.xlabel(\"Month\"); \n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"chart_monthly_events_trend.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Resumo executivo (pequeno)\n",
    "# -----------------------------\n",
    "exec_summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Customers (RFM table)\",\n",
    "        \"Customers modeled\",\n",
    "        \"Total monetary (sum)\",\n",
    "        \"Best segment size\",\n",
    "        \"Months with low anomaly\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        rfm[\"cid\"].nunique() if \"cid\" in rfm.columns else 0,\n",
    "        rfm_model[\"cid\"].nunique() if \"cid\" in rfm_model.columns else 0,\n",
    "        float(np.nansum(rfm_model[\"monetary\"])) if \"monetary\" in rfm_model.columns else 0.0,\n",
    "        int((rfm_model[\"segment\"] == \"Best\").sum()) if \"segment\" in rfm_model.columns else 0,\n",
    "        int(monthly[\"anomaly_low\"].sum()) if \"anomaly_low\" in monthly.columns else 0\n",
    "    ]\n",
    "})\n",
    "exec_summary.to_csv(OUT_DIR / \"executive_summary.csv\", index=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002159b",
   "metadata": {},
   "source": [
    "## Testes de Hipótese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e222ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA (monetary) -> F = 753.477 | p-value = 0.0\n",
      "Arquivos salvos:\n",
      " - Output/anova_results.csv\n",
      " - Output/tukey_results.csv\n"
     ]
    }
   ],
   "source": [
    "# === Testes de Hipótese: ANOVA + Tukey entre clusters (monetary) ===\n",
    "\n",
    "OUT_DIR = Path(\"Output\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Remove NaN/inf\n",
    "df_th = rfm_model[['cluster','monetary']].dropna().copy()\n",
    "df_th = df_th[np.isfinite(df_th['monetary'])]\n",
    "\n",
    "# ANOVA (médias de 'monetary' entre clusters)\n",
    "grupos = [g['monetary'].values for _, g in df_th.groupby('cluster')]\n",
    "anova_F, anova_p = f_oneway(*grupos) if len(grupos) > 1 else (np.nan, np.nan)\n",
    "\n",
    "# Tukey HSD (pareado)\n",
    "# Para Tukey, precisamos de vetor de valores e vetor de grupos\n",
    "if df_th['cluster'].nunique() > 1 and len(df_th) > 0:\n",
    "    tukey = pairwise_tukeyhsd(endog=df_th['monetary'].values,\n",
    "                              groups=df_th['cluster'].astype(str).values,\n",
    "                              alpha=0.05)\n",
    "    tukey_result = pd.DataFrame(tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
    "else:\n",
    "    tukey_result = pd.DataFrame(columns=['group1','group2','meandiff','p-adj','lower','upper','reject'])\n",
    "\n",
    "anova_out = pd.DataFrame({\n",
    "    'metric': ['monetary'],\n",
    "    'F': [anova_F],\n",
    "    'p_value': [anova_p]\n",
    "})\n",
    "anova_out.to_csv(OUT_DIR / \"anova_results.csv\", index=False)\n",
    "\n",
    "tukey_result.to_csv(OUT_DIR / \"tukey_results.csv\", index=False)\n",
    "\n",
    "print(\"ANOVA (monetary) -> F =\", round(anova_F,3), \"| p-value =\", round(anova_p,6))\n",
    "print(\"Arquivos salvos:\")\n",
    "print(\" - Output/anova_results.csv\")\n",
    "print(\" - Output/tukey_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ea590",
   "metadata": {},
   "source": [
    "## Previsão M+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df47b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEOFError: marshal data too short. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"Output\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Carrega a série mensal \n",
    "monthly_path = OUT_DIR / \"monthly_events_trend.csv\"\n",
    "if monthly_path.exists():\n",
    "    monthly_pred = pd.read_csv(monthly_path, parse_dates=['event_dt'], sep=',')\n",
    "    \n",
    "elif 'monthly' in globals():\n",
    "    monthly_pred = monthly.copy()\n",
    "else:\n",
    "    monthly_pred = pd.DataFrame(columns=['month','events'])\n",
    "    print(\"Aviso: monthly_events_trend.csv não encontrado e 'monthly' não existe. Pulo a previsão.\")\n",
    "\n",
    "if not monthly_pred.empty and 'events' in monthly_pred.columns:\n",
    "    # Remove nulos\n",
    "    monthly_pred = monthly_pred[['event_dt','events']].dropna().sort_values('event_dt').reset_index(drop=True)\n",
    "    monthly_pred['t'] = np.arange(len(monthly_pred)) \n",
    "\n",
    "    # Modelo 1: Naive \n",
    "    naive_forecast = monthly_pred['events'].iloc[-1] if len(monthly_pred) > 0 else np.nan\n",
    "\n",
    "    # Modelo 2: Regressão Linear \n",
    "    lr = LinearRegression()\n",
    "    X = monthly_pred[['t']].values\n",
    "    y = monthly_pred['events'].values\n",
    "    if len(monthly_pred) >= 2 and np.isfinite(y).all():\n",
    "        lr.fit(X, y)\n",
    "        next_t = np.array([[monthly_pred['t'].iloc[-1] + 1]])\n",
    "        lr_forecast = float(lr.predict(next_t)[0])\n",
    "    else:\n",
    "        lr_forecast = np.nan\n",
    "\n",
    "    # Próximo mês \n",
    "    if len(monthly_pred) > 0:\n",
    "        next_month = (monthly_pred['event_dt'].iloc[-1] + pd.offsets.MonthBegin(1)).normalize()\n",
    "    else:\n",
    "        next_month = pd.NaT\n",
    "\n",
    "    preds = pd.DataFrame({\n",
    "        'next_month': [next_month],\n",
    "        'naive_events_forecast': [naive_forecast],\n",
    "        'lr_events_forecast': [lr_forecast]\n",
    "    })\n",
    "\n",
    "    preds.to_csv(OUT_DIR / \"predictions_m_plus_1.csv\", index=False)\n",
    "    print(\"Previsões salvas em: Output/predictions_m_plus_1.csv\")\n",
    "else:\n",
    "    print(\"Sem dados mensais de events para prever M+1.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
